{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\7oda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install PyPDF2\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import ast\n",
    "#from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files():\n",
    "    \"\"\"\n",
    "    \n",
    "        return: list of files in the directory \n",
    "    \n",
    "    \"\"\"\n",
    "    directory = \"C:\\\\Users\\\\7oda\\\\New folder\\\\Query-Optimization\\\\files\\\\\"\n",
    "    files_name = []\n",
    "    for sample in os.listdir(directory):\n",
    "        files_name.append(sample)\n",
    "    return files_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order(files_list):\n",
    "    \"\"\"\n",
    "        files_list: list of files we will work on\n",
    "\n",
    "        return: ordered files_list\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    str_temp=''\n",
    "    for i in range(len(files_list)):\n",
    "        pdf = open(\"files/\"+files_list[i],\"rb\")\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "        if(pdf_reader.numPages > temp):\n",
    "            temp = pdf_reader.numPages \n",
    "            str_temp = files_list[i]\n",
    "            del files_list[i]\n",
    "            files_list.insert(0, str_temp)\n",
    "            #print(\"found\"+ files_list[i])\n",
    "            \n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading():\n",
    "    \"\"\"\n",
    "    \n",
    "        return: a dataframe with all the files in columns and the pages of each file in rows\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"=============in the reading =============\")\n",
    "\n",
    "    #files_list =[]\n",
    "    files_list = files()\n",
    "    #make the bigger document in the begenning of the list\n",
    "    files_list = order(files_list)\n",
    "    \n",
    "    #creating a dataframe to hold the docs\n",
    "    df= pd.DataFrame()#\n",
    "\n",
    "    for j in range(len(files_list)):\n",
    "        page_list=[]\n",
    "        print(files_list[j])\n",
    "        pdf = open(\"files/\"+files_list[j],\"rb\")\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "\n",
    "        print(\"num of pages -> \"+ str(pdf_reader.numPages))\n",
    "\n",
    "        for i in range (pdf_reader.numPages):\n",
    "            page = pdf_reader.getPage(i)\n",
    "            pdf_words = page.extractText()\n",
    "            page_list.append(pdf_words)\n",
    "            #print(page_list)\n",
    "        df[files_list[j]] = pd.Series(page_list)\n",
    "        \n",
    "\n",
    "\n",
    "    #closing the pdf file\n",
    "    pdf.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    \"\"\"\n",
    "        df: a dataframe of all the files\n",
    "\n",
    "        return: normalized dataframe with pages index\n",
    "    \"\"\"\n",
    "    print(\"=============in the normalize =============\")\n",
    "    df['pages'] = df.index\n",
    "    #this cell runs only one time\n",
    "    # if you want to run again go back from the previous cell\n",
    "\n",
    "    files_list = files()\n",
    "    stop = stopwords.words('english')\n",
    "    for i in  files_list:\n",
    "        # convert to lower case\n",
    "        df[i] = df[i].str.lower()\n",
    "        # removing puncituations\n",
    "        df[i] = df[i].str.replace('[^\\w\\s]','')\n",
    "        #convert to string\n",
    "        df[i] = df[i].apply(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding(df):\n",
    "    \"\"\"\n",
    "        df: a normalized dataframe of all the files\n",
    "\n",
    "        return: a dataframe with each row represent a line not page, and make lines index\n",
    "    \"\"\"\n",
    "    print(\"=============in the expanding =============\")\n",
    "    new_df=pd.DataFrame()\n",
    "#    global word_list \n",
    "#    global index_list \n",
    "    #loop on each column or file\n",
    "    for i in range (len(df.columns)-1):\n",
    "        word_list = []\n",
    "        index_list = []\n",
    "        page_list = []\n",
    "        index=0\n",
    "        print(\"in the file #\"+str(i))\n",
    "        print(\"===================================\")\n",
    "        #loope on each cell or page\n",
    "        for j in range(len(df.index)):\n",
    "            ptr=-1\n",
    "            #print(j)\n",
    "            if (df.iloc[j][i] == 'nan'):\n",
    "                break\n",
    "            #loop on each element on the cell or character\n",
    "            for k in range(len(df.iloc[j][i])):\n",
    "                if (df.iloc[j][i][k] == '\\n'):\n",
    "                    index_list.append(index)\n",
    "                    word_list.append(df.iloc[j][i][ptr+1:k])\n",
    "                    page_list.append(df.iloc[j][len(df.columns)-1])\n",
    "                    ptr = k\n",
    "                    index=index+1\n",
    "                    #print(k)\n",
    "                    \n",
    "        new_df[str(i)] = pd.Series(word_list)\n",
    "        new_df['file '+ str(i)+' indexes'] = pd.Series(index_list)\n",
    "        new_df['file '+ str(i)+' page numbers'] = pd.Series(page_list)\n",
    "        print(\"length of word_list \"+str(i)+\" is  ---> \"+str(len(word_list)))\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(new_df):\n",
    "    \"\"\"\n",
    "        new_df: dataframe of the files\n",
    "\n",
    "        return: tokenized dataframe\n",
    "    \"\"\"\n",
    "    print(\"=============in the tokenize=============\")\n",
    "    for i in  range(0,10,1):\n",
    "        new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(str)\n",
    "        new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(nltk.word_tokenize)\n",
    "        #new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(lambda x: [item for item in x if item not in stop])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_term_list(new_df):\n",
    "    \"\"\"\n",
    "        new_df: a dataframe of all the files\n",
    "\n",
    "        return: list of lists containing each token with it's doc_id and token_id\n",
    "    \"\"\"\n",
    "    print(\"=============in the return_term_list =============\")\n",
    "    all_terms_list = [[]]\n",
    "    \n",
    "    #loop on each file\n",
    "    for i in range (0, len(new_df.columns), 3):\n",
    "        #loope on each cell or page\n",
    "        counter = 0\n",
    "        for j in range(len(new_df.index)):\n",
    "            if (new_df.iloc[j][i] == ['nan'] or new_df.iloc[j][i] == []):\n",
    "                continue\n",
    "            for k in range(len(new_df.iloc[j][i])):\n",
    "                term_list = []\n",
    "                #term\n",
    "                term_list.append(new_df.iloc[j][i][k])\n",
    "                #doc_id\n",
    "                term_list.append(i/3)\n",
    "                #token_pos in the document\n",
    "                term_list.append(new_df.iloc[j][i+1]+k+counter)\n",
    "                if(k == 1 and len(new_df.iloc[j][i]) == 2):\n",
    "                    counter +=1\n",
    "                elif(k == len(new_df.iloc[j][i])-1):\n",
    "                    counter += k \n",
    "                #page_id\n",
    "                #term_list.append(new_df.iloc[j][i+2])\n",
    "                \n",
    "                #append each term to the all_term_list\n",
    "                all_terms_list.append(term_list)\n",
    "            \n",
    "    return all_terms_list      \n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(all_terms_list):\n",
    "    \"\"\"\n",
    "        all_terms_list: list of lists containing all the terms in all the files\n",
    "\n",
    "        return: list of lists after removing stop words\n",
    "    \"\"\"\n",
    "    new_terms_list = [[]]\n",
    "    print(\"=============in the stop_words_removal=============\")\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    for i in range(len(all_terms_list)):\n",
    "        if (all_terms_list[i] != []):\n",
    "            if not all_terms_list[i][0] in stop:\n",
    "                new_terms_list.append(all_terms_list[i])#.apply(lambda x: [item for item in x if item not in stop])\n",
    "    return new_terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_bitch():\n",
    "    \"\"\"\n",
    "        run all the functions in Pre-Processing\n",
    "    \"\"\"\n",
    "    df = reading()\n",
    "    df = normalize(df)\n",
    "    df = expanding(df)\n",
    "    df = tokenize(df)\n",
    "    all_terms_list = return_term_list(df)\n",
    "    all_terms_list = stop_words_removal(all_terms_list)\n",
    "    return all_terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============in the reading =============\n",
      "Do the Work_ Overcome Resistance and Get Out of Your Own Way ( PDFDrive ).pdf\n",
      "num of pages -> 57\n",
      "Artificial Intelligince.pdf\n",
      "num of pages -> 19\n",
      "7348-dialog-based-interactive-image-retrieval.pdf\n",
      "num of pages -> 11\n",
      "1905.11946v3.pdf\n",
      "num of pages -> 10\n",
      "8299-unsupervised-scale-consistent-depth-and-ego-motion-learning-from-monocular-video(1).pdf\n",
      "num of pages -> 11\n",
      "ImportantTexts.pdf\n",
      "num of pages -> 1\n",
      "liang-j_cvpr2005.pdf\n",
      "num of pages -> 8\n",
      "RH_StudyGuide_V2.pdf\n",
      "num of pages -> 32\n",
      "TextExtractionandCharacterRecognitionform.pdf\n",
      "num of pages -> 5\n",
      "Visualizing_non-speech_sounds_for_the_deaf.pdf\n",
      "num of pages -> 9\n",
      "=============in the normalize =============\n",
      "=============in the expanding =============\n",
      "in the file #0\n",
      "===================================\n",
      "length of word_list 0 is  ---> 2484\n",
      "in the file #1\n",
      "===================================\n",
      "length of word_list 1 is  ---> 300\n",
      "in the file #2\n",
      "===================================\n",
      "length of word_list 2 is  ---> 1418\n",
      "in the file #3\n",
      "===================================\n",
      "length of word_list 3 is  ---> 2283\n",
      "in the file #4\n",
      "===================================\n",
      "length of word_list 4 is  ---> 1581\n",
      "in the file #5\n",
      "===================================\n",
      "length of word_list 5 is  ---> 56\n",
      "in the file #6\n",
      "===================================\n",
      "length of word_list 6 is  ---> 1682\n",
      "in the file #7\n",
      "===================================\n",
      "length of word_list 7 is  ---> 554\n",
      "in the file #8\n",
      "===================================\n",
      "length of word_list 8 is  ---> 1037\n",
      "in the file #9\n",
      "===================================\n",
      "length of word_list 9 is  ---> 1289\n",
      "=============in the tokenize=============\n",
      "=============in the return_term_list =============\n",
      "=============in the stop_words_removal=============\n"
     ]
    }
   ],
   "source": [
    "all_terms_list = run_bitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017', 2.0, 1281.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_list[9980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['pages'] = df.index\n",
    "#this cell runs only one time\n",
    "# if you want to run again go back from the previous cell\n",
    "\n",
    "#files_list = files()\n",
    "#stop = stopwords.words('english')\n",
    "#for i in  files_list:\n",
    "    # convert to lower case\n",
    "  #  df[i] = df[i].str.lower()\n",
    "    # removing puncituations\n",
    " #   df[i] = df[i].str.replace('[^\\w\\s]','')\n",
    "    #convert to string\n",
    "#    df[i] = df[i].apply(str)\n",
    "    \n",
    "\n",
    "    #splitting new lines on '\\n'\n",
    "#    for j in range(len(df.index)):\n",
    "#        df[i][j] = df[i][j].replace('\\r', ' ').split('\\n')\n",
    "    #df[i] = df[i].apply(literal_eval)    \n",
    "    #df[i] = df[i].apply(str)\n",
    "\n",
    "    #df.explode(i)\n",
    "\n",
    "# convert list of pd.Series then stack it\n",
    "#    df = (df\n",
    "     #.set_index(['issue_key','date','pkey','case_count'])['component']\n",
    "#     .apply(pd.Series)\n",
    "#     .stack()\n",
    "#     .reset_index()\n",
    "#     .reset_index(level=0, drop=True))\n",
    "     #.rename(columns={0:'component'}))\n",
    "\n",
    "    #convert to string\n",
    "    #df[i] = df[i].apply(str)\n",
    "    \n",
    "    # tokenize\n",
    "    #df[i] = df[i].apply(nltk.word_tokenize)\n",
    "    \n",
    "    # removing stop words\n",
    "    #df[i].apply(lambda x: [item for item in x if item not in stop])\n",
    "    \n",
    "    #df[i]= df[i].str.splitlines()\n",
    "    #df[i].apply(lambda x: (word for word in x.str.splitlines()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
