{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\7oda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install PyPDF2\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import ast\n",
    "#from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files():\n",
    "    \"\"\"\n",
    "    \n",
    "        return: list of files in the directory \n",
    "    \n",
    "    \"\"\"\n",
    "    directory = \"C:\\\\Users\\\\7oda\\\\New folder\\\\Query-Optimization\\\\files\\\\\"\n",
    "    files_name = []\n",
    "    for sample in os.listdir(directory):\n",
    "        files_name.append(sample)\n",
    "    return files_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order(files_list):\n",
    "    \"\"\"\n",
    "        files_list: list of files we will work on\n",
    "\n",
    "        return: ordered files_list\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    str_temp=''\n",
    "    for i in range(len(files_list)):\n",
    "        pdf = open(\"files/\"+files_list[i],\"rb\")\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "        if(pdf_reader.numPages > temp):\n",
    "            temp = pdf_reader.numPages \n",
    "            str_temp = files_list[i]\n",
    "            del files_list[i]\n",
    "            files_list.insert(0, str_temp)\n",
    "            #print(\"found\"+ files_list[i])\n",
    "            \n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading():\n",
    "    \"\"\"\n",
    "    \n",
    "        return: a dataframe with all the files in columns and the pages of each file in rows\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"=============in the reading =============\")\n",
    "\n",
    "    #files_list =[]\n",
    "    files_list = files()\n",
    "    #make the bigger document in the begenning of the list\n",
    "    files_list = order(files_list)\n",
    "    \n",
    "    #creating a dataframe to hold the docs\n",
    "    df= pd.DataFrame()#\n",
    "\n",
    "    for j in range(len(files_list)):\n",
    "        page_list=[]\n",
    "        print(files_list[j])\n",
    "        pdf = open(\"files/\"+files_list[j],\"rb\")\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "\n",
    "        print(\"num of pages -> \"+ str(pdf_reader.numPages))\n",
    "\n",
    "        for i in range (pdf_reader.numPages):\n",
    "            page = pdf_reader.getPage(i)\n",
    "            pdf_words = page.extractText()\n",
    "            page_list.append(pdf_words)\n",
    "            #print(page_list)\n",
    "        df[files_list[j]] = pd.Series(page_list)\n",
    "        \n",
    "\n",
    "\n",
    "    #closing the pdf file\n",
    "    pdf.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    \"\"\"\n",
    "        df: a dataframe of all the files\n",
    "\n",
    "        return: normalized dataframe with pages index\n",
    "    \"\"\"\n",
    "    print(\"=============in the normalize =============\")\n",
    "    df['pages'] = df.index\n",
    "    #this cell runs only one time\n",
    "    # if you want to run again go back from the previous cell\n",
    "\n",
    "    files_list = files()\n",
    "    stop = stopwords.words('english')\n",
    "    for i in  files_list:\n",
    "        # convert to lower case\n",
    "        df[i] = df[i].str.lower()\n",
    "        # removing puncituations\n",
    "        df[i] = df[i].str.replace('[^\\w\\s]','')\n",
    "        #convert to string\n",
    "        df[i] = df[i].apply(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding(df):\n",
    "    \"\"\"\n",
    "        df: a normalized dataframe of all the files\n",
    "\n",
    "        return: a dataframe with each row represent a line not page, and make lines index\n",
    "    \"\"\"\n",
    "    print(\"=============in the expanding =============\")\n",
    "    new_df=pd.DataFrame()\n",
    "#    global word_list \n",
    "#    global index_list \n",
    "    #loop on each column or file\n",
    "    for i in range (len(df.columns)-1):\n",
    "        word_list = []\n",
    "        index_list = []\n",
    "        page_list = []\n",
    "        index=0\n",
    "        print(\"in the file #\"+str(i))\n",
    "        print(\"===================================\")\n",
    "        #loope on each cell or page\n",
    "        for j in range(len(df.index)):\n",
    "            ptr=-1\n",
    "            #print(j)\n",
    "            if (df.iloc[j][i] == 'nan'):\n",
    "                break\n",
    "            #loop on each element on the cell or character\n",
    "            for k in range(len(df.iloc[j][i])):\n",
    "                if (df.iloc[j][i][k] == '\\n'):\n",
    "                    index_list.append(index)\n",
    "                    word_list.append(df.iloc[j][i][ptr+1:k])\n",
    "                    page_list.append(df.iloc[j][len(df.columns)-1])\n",
    "                    ptr = k\n",
    "                    index=index+1\n",
    "                    #print(k)\n",
    "                    \n",
    "        new_df[str(i)] = pd.Series(word_list)\n",
    "        new_df['file '+ str(i)+' indexes'] = pd.Series(index_list)\n",
    "        new_df['file '+ str(i)+' page numbers'] = pd.Series(page_list)\n",
    "        print(\"length of word_list \"+str(i)+\" is  ---> \"+str(len(word_list)))\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(new_df):\n",
    "    \"\"\"\n",
    "        new_df: dataframe of the files\n",
    "\n",
    "        return: tokenized dataframe\n",
    "    \"\"\"\n",
    "    print(\"=============in the tokenize=============\")\n",
    "    for i in  range(0,10,1):\n",
    "        new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(str)\n",
    "        new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(nltk.word_tokenize)\n",
    "        #new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(lambda x: [item for item in x if item not in stop])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_term_list(new_df):\n",
    "    \"\"\"\n",
    "        new_df: a dataframe of all the files\n",
    "\n",
    "        return: list of lists containing each token with it's doc_id and token_id\n",
    "    \"\"\"\n",
    "    print(\"=============in the return_term_list =============\")\n",
    "    all_terms_list = [[]]\n",
    "    \n",
    "    #loop on each file\n",
    "    for i in range (0, len(new_df.columns), 3):\n",
    "        #loope on each cell or page\n",
    "        counter = 0\n",
    "        for j in range(len(new_df.index)):\n",
    "            if (new_df.iloc[j][i] == ['nan'] or new_df.iloc[j][i] == []):\n",
    "                continue\n",
    "            for k in range(len(new_df.iloc[j][i])):\n",
    "                term_list = []\n",
    "                #term\n",
    "                term_list.append(new_df.iloc[j][i][k])\n",
    "                #doc_id\n",
    "                term_list.append(i/3)\n",
    "                #token_pos in the document\n",
    "                term_list.append(new_df.iloc[j][i+1]+k+counter)\n",
    "                if(k == 1 and len(new_df.iloc[j][i]) == 2):\n",
    "                    counter +=1\n",
    "                elif(k == len(new_df.iloc[j][i])-1):\n",
    "                    counter += k \n",
    "                #page_id\n",
    "                #term_list.append(new_df.iloc[j][i+2])\n",
    "                \n",
    "                #append each term to the all_term_list\n",
    "                all_terms_list.append(term_list)\n",
    "            \n",
    "    return all_terms_list      \n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(all_terms_list):\n",
    "    \"\"\"\n",
    "        all_terms_list: list of lists containing all the terms in all the files\n",
    "\n",
    "        return: list of lists after removing stop words\n",
    "    \"\"\"\n",
    "    new_terms_list = [[]]\n",
    "    print(\"=============in the stop_words_removal=============\")\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    for i in range(len(all_terms_list)):\n",
    "        if (all_terms_list[i] != []):\n",
    "            if not all_terms_list[i][0] in stop:\n",
    "                new_terms_list.append(all_terms_list[i])#.apply(lambda x: [item for item in x if item not in stop])\n",
    "    return new_terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============in the reading =============\n",
      "Do the Work_ Overcome Resistance and Get Out of Your Own Way ( PDFDrive ).pdf\n",
      "57\n",
      "Artificial Intelligince.pdf\n",
      "19\n",
      "7348-dialog-based-interactive-image-retrieval.pdf\n",
      "11\n",
      "1905.11946v3.pdf\n",
      "10\n",
      "8299-unsupervised-scale-consistent-depth-and-ego-motion-learning-from-monocular-video(1).pdf\n",
      "11\n",
      "ImportantTexts.pdf\n",
      "1\n",
      "RH_StudyGuide_V2.pdf\n",
      "32\n",
      "sample.pdf\n",
      "2\n",
      "TextExtractionandCharacterRecognitionform.pdf\n",
      "5\n",
      "Visualizing_non-speech_sounds_for_the_deaf.pdf\n",
      "9\n",
      "=============in the normalize =============\n",
      "=============in the expanding =============\n",
      "in the file #0\n",
      "===================================\n",
      "length of word_list 0 is  ---> 2484\n",
      "in the file #1\n",
      "===================================\n",
      "length of word_list 1 is  ---> 300\n",
      "in the file #2\n",
      "===================================\n",
      "length of word_list 2 is  ---> 1418\n",
      "in the file #3\n",
      "===================================\n",
      "length of word_list 3 is  ---> 2283\n",
      "in the file #4\n",
      "===================================\n",
      "length of word_list 4 is  ---> 1581\n",
      "in the file #5\n",
      "===================================\n",
      "length of word_list 5 is  ---> 56\n",
      "in the file #6\n",
      "===================================\n",
      "length of word_list 6 is  ---> 554\n",
      "in the file #7\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning. [ipykernel_launcher.py:35]\n",
      "DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning. [ipykernel_launcher.py:36]\n",
      "DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning. [ipykernel_launcher.py:37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_list 7 is  ---> 0\n",
      "in the file #8\n",
      "===================================\n",
      "length of word_list 8 is  ---> 1037\n",
      "in the file #9\n",
      "===================================\n",
      "length of word_list 9 is  ---> 1289\n",
      "=============in the tokenize=============\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-4ae931cc6a1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpanding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mall_terms_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_term_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mall_terms_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words_removal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_terms_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-b40e58e72ae1>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(new_df)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#new_df.loc[:][str(i)] = new_df.loc[:][str(i)].apply(lambda x: [item for item in x if item not in stop])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "df = reading()\n",
    "df = normalize(df)\n",
    "df = expanding(df)\n",
    "df = tokenize(df)\n",
    "all_terms_list = return_term_list(df)\n",
    "all_terms_list = stop_words_removal(all_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['work', 0.0, 2],\n",
       " ['work', 0.0, 5],\n",
       " ['overcome', 0.0, 6],\n",
       " ['resistance', 0.0, 7],\n",
       " ['get', 0.0, 9],\n",
       " ['way', 0.0, 14],\n",
       " ['eld', 0.0, 15],\n",
       " ['author', 0.0, 16],\n",
       " ['war', 0.0, 19],\n",
       " ['art', 0.0, 21],\n",
       " ['also', 0.0, 22],\n",
       " ['steven', 0.0, 24],\n",
       " ['pres', 0.0, 25],\n",
       " ['eld', 0.0, 26],\n",
       " ['fiction', 0.0, 27],\n",
       " ['e', 0.0, 28],\n",
       " ['profession', 0.0, 29],\n",
       " ['killing', 0.0, 30],\n",
       " ['rommel', 0.0, 31],\n",
       " ['e', 0.0, 32],\n",
       " ['afghan', 0.0, 33],\n",
       " ['campaign', 0.0, 34],\n",
       " ['e', 0.0, 35],\n",
       " ['virtues', 0.0, 36],\n",
       " ['war', 0.0, 38],\n",
       " ['last', 0.0, 39],\n",
       " ['amazons', 0.0, 42],\n",
       " ['tides', 0.0, 43],\n",
       " ['war', 0.0, 45],\n",
       " ['gates', 0.0, 46],\n",
       " ['fire', 0.0, 48],\n",
       " ['e', 0.0, 49],\n",
       " ['legend', 0.0, 50],\n",
       " ['bagger', 0.0, 52],\n",
       " ['vance', 0.0, 53],\n",
       " ['n', 0.0, 54],\n",
       " ['ction', 0.0, 55],\n",
       " ['e', 0.0, 56],\n",
       " ['lions', 0.0, 57],\n",
       " ['gate', 0.0, 58],\n",
       " ['e', 0.0, 59],\n",
       " ['authentic', 0.0, 60],\n",
       " ['swing', 0.0, 61],\n",
       " ['turning', 0.0, 62],\n",
       " ['pro', 0.0, 63],\n",
       " ['e', 0.0, 64],\n",
       " ['warrior', 0.0, 65],\n",
       " ['ethos', 0.0, 66],\n",
       " ['e', 0.0, 67],\n",
       " ['war', 0.0, 68],\n",
       " ['art', 0.0, 70],\n",
       " ['work', 0.0, 73],\n",
       " ['2011', 0.0, 74],\n",
       " ['steven', 0.0, 75],\n",
       " ['pres', 0.0, 76],\n",
       " ['eld', 0.0, 77],\n",
       " ['e', 0.0, 78],\n",
       " ['domino', 0.0, 79],\n",
       " ['project', 0.0, 80],\n",
       " ['published', 0.0, 81],\n",
       " ['zoom', 0.0, 85],\n",
       " ['inc', 0.0, 86],\n",
       " ['black', 0.0, 88],\n",
       " ['irish', 0.0, 89],\n",
       " ['entertainment', 0.0, 90],\n",
       " ['llc', 0.0, 91],\n",
       " ['sign', 0.0, 93],\n",
       " ['updates', 0.0, 96],\n",
       " ['free', 0.0, 98],\n",
       " ['stu', 0.0, 99],\n",
       " ['wwwthedominoprojectcom', 0.0, 101],\n",
       " ['library', 0.0, 102],\n",
       " ['congress', 0.0, 104],\n",
       " ['cataloging', 0.0, 105],\n",
       " ['publication', 0.0, 107],\n",
       " ['data', 0.0, 108],\n",
       " ['press', 0.0, 110],\n",
       " ['eld', 0.0, 111],\n",
       " ['steven', 0.0, 112],\n",
       " ['1943', 0.0, 113],\n",
       " ['e', 0.0, 115],\n",
       " ['work', 0.0, 116],\n",
       " ['overcome', 0.0, 117],\n",
       " ['resistance', 0.0, 118],\n",
       " ['get', 0.0, 120],\n",
       " ['way', 0.0, 125],\n",
       " ['steven', 0.0, 126],\n",
       " ['pres', 0.0, 127],\n",
       " ['eld', 0.0, 128],\n",
       " ['p', 0.0, 129],\n",
       " ['cm', 0.0, 130],\n",
       " ['isbn', 0.0, 132],\n",
       " ['9781936891320', 0.0, 133],\n",
       " ['printed', 0.0, 135],\n",
       " ['united', 0.0, 138],\n",
       " ['states', 0.0, 139],\n",
       " ['america', 0.0, 141],\n",
       " ['ellie', 0.0, 143],\n",
       " ['manifesto', 0.0, 146],\n",
       " ['steve', 0.0, 147],\n",
       " ['gets', 0.0, 148],\n",
       " ['practical', 0.0, 149],\n",
       " ['direct', 0.0, 150],\n",
       " ['personal', 0.0, 152],\n",
       " ['read', 0.0, 153],\n",
       " ['fast', 0.0, 155],\n",
       " ['read', 0.0, 157],\n",
       " ['take', 0.0, 161],\n",
       " ['notes', 0.0, 162],\n",
       " ['en', 0.0, 163],\n",
       " ['buy', 0.0, 164],\n",
       " ['copy', 0.0, 166],\n",
       " ['everyone', 0.0, 168],\n",
       " ['else', 0.0, 169],\n",
       " ['whos', 0.0, 170],\n",
       " ['stuck', 0.0, 171],\n",
       " ['push', 0.0, 173],\n",
       " ['get', 0.0, 176],\n",
       " ['work', 0.0, 178],\n",
       " ['well', 0.0, 180],\n",
       " ['hurry', 0.0, 181],\n",
       " ['seth', 0.0, 184],\n",
       " ['godin', 0.0, 185],\n",
       " ['hastingsonhudson', 0.0, 186],\n",
       " ['january', 0.0, 187],\n",
       " ['2011', 0.0, 188],\n",
       " ['foreword', 0.0, 189],\n",
       " ['right', 0.0, 190],\n",
       " ['driveway', 0.0, 194],\n",
       " ['really', 0.0, 197],\n",
       " ['fast', 0.0, 198],\n",
       " ['car', 0.0, 199],\n",
       " ['one', 0.0, 201],\n",
       " ['stupid', 0.0, 205],\n",
       " ['hamptonsstyle', 0.0, 206],\n",
       " ['richguy', 0.0, 207],\n",
       " ['showy', 0.0, 208],\n",
       " ['cars', 0.0, 209],\n",
       " ['like', 0.0, 210],\n",
       " ['ferrari', 0.0, 212],\n",
       " ['honest', 0.0, 216],\n",
       " ['fast', 0.0, 217],\n",
       " ['car', 0.0, 218],\n",
       " ['perhaps', 0.0, 219],\n",
       " ['subaru', 0.0, 221],\n",
       " ['wrx', 0.0, 222],\n",
       " ['keys', 0.0, 228],\n",
       " ['go', 0.0, 230],\n",
       " ['drive', 0.0, 231],\n",
       " ['right', 0.0, 233],\n",
       " ['runway', 0.0, 237],\n",
       " ['private', 0.0, 240],\n",
       " ['jet', 0.0, 241],\n",
       " ['r', 0.0, 242],\n",
       " ['wher', 0.0, 245],\n",
       " ['ever', 0.0, 246],\n",
       " ['want', 0.0, 248],\n",
       " ['go', 0.0, 250],\n",
       " ['heres', 0.0, 251],\n",
       " ['pilot', 0.0, 253],\n",
       " ['standing', 0.0, 254],\n",
       " ['go', 0.0, 256],\n",
       " ['leave', 0.0, 257],\n",
       " ['right', 0.0, 258],\n",
       " ['hand', 0.0, 262],\n",
       " ['chicago', 0.0, 265],\n",
       " ['pneumatics', 0.0, 266],\n",
       " ['0651', 0.0, 267],\n",
       " ['ham', 0.0, 268],\n",
       " ['mer', 0.0, 269],\n",
       " ['drive', 0.0, 272],\n",
       " ['nail', 0.0, 274],\n",
       " ['anything', 0.0, 278],\n",
       " ['choose', 0.0, 287],\n",
       " ['time', 0.0, 288],\n",
       " ['use', 0.0, 290],\n",
       " ['heres', 0.0, 293],\n",
       " ['keyboard', 0.0, 295],\n",
       " ['connected', 0.0, 296],\n",
       " ['entire', 0.0, 299],\n",
       " ['world', 0.0, 300],\n",
       " ['heres', 0.0, 301],\n",
       " ['publishing', 0.0, 303],\n",
       " ['platform', 0.0, 304],\n",
       " ['use', 0.0, 307],\n",
       " ['interact', 0.0, 309],\n",
       " ['anyone', 0.0, 314],\n",
       " ['time', 0.0, 318],\n",
       " ['free', 0.0, 320],\n",
       " ['wanted', 0.0, 322],\n",
       " ['level', 0.0, 324],\n",
       " ['play', 0.0, 325],\n",
       " ['ing', 0.0, 327],\n",
       " ['eld', 0.0, 328],\n",
       " ['one', 0.0, 329],\n",
       " ['good', 0.0, 335],\n",
       " ['shot', 0.0, 337],\n",
       " ['anyone', 0.0, 339],\n",
       " ['else', 0.0, 340],\n",
       " ['work', 0.0, 346],\n",
       " ['ats', 0.0, 347],\n",
       " ['waiting', 0.0, 351],\n",
       " ['doto', 0.0, 355],\n",
       " ['work', 0.0, 358],\n",
       " ['steven', 0.0, 359],\n",
       " ['press', 0.0, 360],\n",
       " ['eld', 0.0, 361],\n",
       " ['author', 0.0, 364],\n",
       " ['important', 0.0, 368],\n",
       " ['book', 0.0, 369],\n",
       " ['youve', 0.0, 370],\n",
       " ['never', 0.0, 371],\n",
       " ['read', 0.0, 372],\n",
       " ['e', 0.0, 373],\n",
       " ['war', 0.0, 374],\n",
       " ['art', 0.0, 376],\n",
       " ['help', 0.0, 379],\n",
       " ['understand', 0.0, 381],\n",
       " ['youre', 0.0, 383],\n",
       " ['stuck', 0.0, 384],\n",
       " ['kick', 0.0, 387],\n",
       " ['pants', 0.0, 391],\n",
       " ['get', 0.0, 395],\n",
       " ['moving', 0.0, 398],\n",
       " ['must', 0.0, 403],\n",
       " ['buy', 0.0, 404],\n",
       " ['copy', 0.0, 406],\n",
       " ['soon', 0.0, 408],\n",
       " ['nish', 0.0, 412],\n",
       " ['reading', 0.0, 413],\n",
       " ['book', 0.0, 417],\n",
       " ['book', 0.0, 419],\n",
       " ['designed', 0.0, 421],\n",
       " ['coach', 0.0, 423],\n",
       " ['project', 0.0, 427],\n",
       " ['book', 0.0, 429],\n",
       " ['ballet', 0.0, 431],\n",
       " ['new', 0.0, 433],\n",
       " ['business', 0.0, 434],\n",
       " ['venture', 0.0, 435],\n",
       " ['philanthropic', 0.0, 437],\n",
       " ['enterprise', 0.0, 438],\n",
       " ['concepti', 0.0, 441],\n",
       " ['nished', 0.0, 442],\n",
       " ['product', 0.0, 443],\n",
       " ['seeing', 0.0, 444],\n",
       " ['point', 0.0, 448],\n",
       " ['view', 0.0, 450],\n",
       " ['resistance', 0.0, 452],\n",
       " ['well', 0.0, 453],\n",
       " ['hit', 0.0, 454],\n",
       " ['every', 0.0, 455],\n",
       " ['predictable', 0.0, 456],\n",
       " ['resistance', 0.0, 457],\n",
       " ['point', 0.0, 458],\n",
       " ['along', 0.0, 459],\n",
       " ['way', 0.0, 461],\n",
       " ['junctures', 0.0, 463],\n",
       " ['fear', 0.0, 465],\n",
       " ['selfsabotage', 0.0, 466],\n",
       " ['procrastination', 0.0, 467],\n",
       " ['self', 0.0, 468],\n",
       " ['doubt', 0.0, 470],\n",
       " ['demons', 0.0, 475],\n",
       " ['familiar', 0.0, 479],\n",
       " ['counted', 0.0, 484],\n",
       " ['upon', 0.0, 485],\n",
       " ['strike', 0.0, 487],\n",
       " ['butts', 0.0, 489],\n",
       " ['need', 0.0, 490],\n",
       " ['kicked', 0.0, 493],\n",
       " ['shall', 0.0, 495],\n",
       " ['kick', 0.0, 496],\n",
       " ['kinder', 0.0, 499],\n",
       " ['gentler', 0.0, 500],\n",
       " ['methods', 0.0, 501],\n",
       " ['called', 0.0, 503],\n",
       " ['well', 0.0, 505],\n",
       " ['get', 0.0, 506],\n",
       " ['kid', 0.0, 509],\n",
       " ['gloves', 0.0, 510],\n",
       " ['one', 0.0, 511],\n",
       " ['note', 0.0, 512],\n",
       " ['document', 0.0, 514],\n",
       " ['articulated', 0.0, 516],\n",
       " ['part', 0.0, 520],\n",
       " ['lexicon', 0.0, 523],\n",
       " ['writerie', 0.0, 526],\n",
       " ['model', 0.0, 528],\n",
       " ['used', 0.0, 529],\n",
       " ['conceiving', 0.0, 533],\n",
       " ['constructing', 0.0, 536],\n",
       " ['plays', 0.0, 537],\n",
       " ['novels', 0.0, 538],\n",
       " ['screenplays', 0.0, 540],\n",
       " ['principles', 0.0, 543],\n",
       " ['applied', 0.0, 547],\n",
       " ['equa', 0.0, 549],\n",
       " ['ectiveness', 0.0, 550],\n",
       " ['form', 0.0, 553],\n",
       " ['creative', 0.0, 555],\n",
       " ['endeavor', 0.0, 556],\n",
       " ['including', 0.0, 557],\n",
       " ['seemingly', 0.0, 559],\n",
       " ['far', 0.0, 560],\n",
       " ['eld', 0.0, 561],\n",
       " ['enterprises', 0.0, 562],\n",
       " ['th', 0.0, 566],\n",
       " ['eld', 0.0, 567],\n",
       " ['self', 0.0, 570],\n",
       " ['stand', 0.0, 571],\n",
       " ['knight', 0.0, 573],\n",
       " ['dragon', 0.0, 576],\n",
       " ['knight', 0.0, 580],\n",
       " ['resistance', 0.0, 581],\n",
       " ['dragon', 0.0, 584],\n",
       " ['steven', 0.0, 585],\n",
       " ['p', 0.0, 586],\n",
       " ['elddo', 0.0, 587],\n",
       " ['e', 0.0, 588],\n",
       " ['work', 0.0, 589],\n",
       " ['23acquisition', 0.0, 590],\n",
       " ['physical', 0.0, 592],\n",
       " ['tness', 0.0, 593],\n",
       " ['recovery', 0.0, 595],\n",
       " ['broken', 0.0, 598],\n",
       " ['heart', 0.0, 599],\n",
       " ['pursuit', 0.0, 602],\n",
       " ['objectiveemotional', 0.0, 605],\n",
       " ['intellectual', 0.0, 606],\n",
       " ['spir', 0.0, 608],\n",
       " ['itualthat', 0.0, 610],\n",
       " ['involves', 0.0, 611],\n",
       " ['moving', 0.0, 612],\n",
       " ['lower', 0.0, 615],\n",
       " ['less', 0.0, 617],\n",
       " ['conscious', 0.0, 618],\n",
       " ['plane', 0.0, 619],\n",
       " ['higher', 0.0, 623],\n",
       " ['one', 0.0, 624],\n",
       " ['orientation', 0.0, 625],\n",
       " ['steven', 0.0, 626],\n",
       " ['p', 0.0, 627],\n",
       " ['elddo', 0.0, 628],\n",
       " ['e', 0.0, 629],\n",
       " ['work', 0.0, 630],\n",
       " ['45our', 0.0, 631],\n",
       " ['enemies', 0.0, 632],\n",
       " ['e', 0.0, 633],\n",
       " ['following', 0.0, 634],\n",
       " ['list', 0.0, 637],\n",
       " ['forces', 0.0, 640],\n",
       " ['arrayed', 0.0, 641],\n",
       " ['us', 0.0, 643],\n",
       " ['artists', 0.0, 645],\n",
       " ['entrepreneurs', 0.0, 647],\n",
       " ['1', 0.0, 648],\n",
       " ['resistance', 0.0, 649],\n",
       " ['ie', 0.0, 650],\n",
       " ['fear', 0.0, 651],\n",
       " ['selfdoubt', 0.0, 652],\n",
       " ['procrastination', 0.0, 653],\n",
       " ['addiction', 0.0, 654],\n",
       " ['distraction', 0.0, 655],\n",
       " ['timidity', 0.0, 656],\n",
       " ['ego', 0.0, 657],\n",
       " ['narcissism', 0.0, 659],\n",
       " ['self', 0.0, 660],\n",
       " ['loathing', 0.0, 662],\n",
       " ['perfectionism', 0.0, 663],\n",
       " ['etc', 0.0, 664],\n",
       " ['2', 0.0, 665],\n",
       " ['rational', 0.0, 666],\n",
       " ['thought', 0.0, 667],\n",
       " ['3', 0.0, 669],\n",
       " ['friends', 0.0, 670],\n",
       " ['family', 0.0, 672],\n",
       " ['resistancewhat', 0.0, 673],\n",
       " ['exactly', 0.0, 674],\n",
       " ['monster', 0.0, 677],\n",
       " ['e', 0.0, 678],\n",
       " ['following', 0.0, 679],\n",
       " ['chapters', 0.0, 681],\n",
       " ['e', 0.0, 683],\n",
       " ['war', 0.0, 684],\n",
       " ['art', 0.0, 686],\n",
       " ['bring', 0.0, 688],\n",
       " ['us', 0.0, 689],\n",
       " ['speed', 0.0, 692],\n",
       " ['resistances', 0.0, 693],\n",
       " ['greatest', 0.0, 694],\n",
       " ['hits', 0.0, 695],\n",
       " ['e', 0.0, 696],\n",
       " ['following', 0.0, 697],\n",
       " ['list', 0.0, 700],\n",
       " ['particular', 0.0, 703],\n",
       " ['order', 0.0, 704],\n",
       " ['activities', 0.0, 707],\n",
       " ['commonly', 0.0, 710],\n",
       " ['elicit', 0.0, 711],\n",
       " ['resistance', 0.0, 712],\n",
       " ['1', 0.0, 713],\n",
       " ['e', 0.0, 714],\n",
       " ['pursuit', 0.0, 715],\n",
       " ['calling', 0.0, 718],\n",
       " ['writing', 0.0, 720],\n",
       " ['painting', 0.0, 721],\n",
       " ['music', 0.0, 722],\n",
       " ['lm', 0.0, 723],\n",
       " ['dance', 0.0, 724],\n",
       " ['creative', 0.0, 727],\n",
       " ['art', 0.0, 728],\n",
       " ['however', 0.0, 729],\n",
       " ['marginal', 0.0, 730],\n",
       " ['unconventional', 0.0, 732],\n",
       " ['2', 0.0, 733],\n",
       " ['e', 0.0, 734],\n",
       " ['launching', 0.0, 735],\n",
       " ['entrepreneurial', 0.0, 738],\n",
       " ['venture', 0.0, 739],\n",
       " ['enterprise', 0.0, 741],\n",
       " ['pr', 0.0, 743],\n",
       " ['otherwise', 0.0, 746],\n",
       " ['3', 0.0, 747],\n",
       " ['diet', 0.0, 749],\n",
       " ['health', 0.0, 751],\n",
       " ['regimen', 0.0, 752],\n",
       " ['4', 0.0, 754],\n",
       " ['program', 0.0, 756],\n",
       " ['spiritual', 0.0, 758],\n",
       " ['advancement', 0.0, 759],\n",
       " ['5', 0.0, 760],\n",
       " ['activity', 0.0, 762],\n",
       " ['whose', 0.0, 763],\n",
       " ['aim', 0.0, 764],\n",
       " ['acquisition', 0.0, 767],\n",
       " ['chiseled', 0.0, 769],\n",
       " ['abdominals', 0.0, 770],\n",
       " ['6', 0.0, 771],\n",
       " ['course', 0.0, 773],\n",
       " ['program', 0.0, 775],\n",
       " ['designed', 0.0, 776],\n",
       " ['overcome', 0.0, 778],\n",
       " ['unwholesome', 0.0, 780],\n",
       " ['habit', 0.0, 781],\n",
       " ['addiction', 0.0, 783],\n",
       " ['7', 0.0, 784],\n",
       " ['education', 0.0, 785],\n",
       " ['every', 0.0, 787],\n",
       " ['kind', 0.0, 788],\n",
       " ['8', 0.0, 790],\n",
       " ['act', 0.0, 792],\n",
       " ['political', 0.0, 794],\n",
       " ['moral', 0.0, 795],\n",
       " ['ethical', 0.0, 797],\n",
       " ['courage', 0.0, 798],\n",
       " ['including', 0.0, 799],\n",
       " ['decision', 0.0, 801],\n",
       " ['change', 0.0, 803],\n",
       " ['better', 0.0, 806],\n",
       " ['unworthy', 0.0, 808],\n",
       " ['pattern', 0.0, 810],\n",
       " ['thought', 0.0, 812],\n",
       " ['conduct', 0.0, 814],\n",
       " ['9', 0.0, 817],\n",
       " ['e', 0.0, 818],\n",
       " ['undertaking', 0.0, 819],\n",
       " ['enterprise', 0.0, 822],\n",
       " ['endeavor', 0.0, 824],\n",
       " ['whose', 0.0, 825],\n",
       " ['aim', 0.0, 826],\n",
       " ['help', 0.0, 829],\n",
       " ['others', 0.0, 830],\n",
       " ['10', 0.0, 831],\n",
       " ['act', 0.0, 833],\n",
       " ['entails', 0.0, 835],\n",
       " ['commitment', 0.0, 836],\n",
       " ['heartthe', 0.0, 839],\n",
       " ['decision', 0.0, 840],\n",
       " ['get', 0.0, 842],\n",
       " ['married', 0.0, 843],\n",
       " ['child', 0.0, 847],\n",
       " ['weather', 0.0, 849],\n",
       " ['rocky', 0.0, 852],\n",
       " ['patch', 0.0, 853],\n",
       " ['relationship', 0.0, 856],\n",
       " ['11', 0.0, 857],\n",
       " ['e', 0.0, 858],\n",
       " ['taking', 0.0, 859],\n",
       " ['principled', 0.0, 862],\n",
       " ['stand', 0.0, 863],\n",
       " ['face', 0.0, 866],\n",
       " ['adversity', 0.0, 868],\n",
       " ['words', 0.0, 871],\n",
       " ['act', 0.0, 873],\n",
       " ['rejects', 0.0, 875],\n",
       " ['immediate', 0.0, 876],\n",
       " ['grat', 0.0, 877],\n",
       " ['cation', 0.0, 878],\n",
       " ['favor', 0.0, 880],\n",
       " ['longterm', 0.0, 882],\n",
       " ['growth', 0.0, 883],\n",
       " ['health', 0.0, 884],\n",
       " ['integrity', 0.0, 886],\n",
       " ['expressed', 0.0, 888],\n",
       " ['another', 0.0, 889],\n",
       " ['way', 0.0, 890],\n",
       " ['act', 0.0, 892],\n",
       " ['derives', 0.0, 894],\n",
       " ['higher', 0.0, 897],\n",
       " ['nature', 0.0, 898],\n",
       " ['instead', 0.0, 899],\n",
       " ['lower', 0.0, 902],\n",
       " ['acts', 0.0, 906],\n",
       " ['elicit', 0.0, 908],\n",
       " ['resistance', 0.0, 910],\n",
       " ['characteristics', 0.0, 915],\n",
       " ['resistance', 0.0, 917],\n",
       " ['steven', 0.0, 918],\n",
       " ['p', 0.0, 919],\n",
       " ['elddo', 0.0, 920],\n",
       " ['e', 0.0, 921],\n",
       " ['work', 0.0, 922],\n",
       " ['67resistance', 0.0, 923],\n",
       " ['invisible', 0.0, 925],\n",
       " ['resistance', 0.0, 926],\n",
       " ['seen', 0.0, 930],\n",
       " ['heard', 0.0, 931],\n",
       " ['touched', 0.0, 932],\n",
       " ['smelled', 0.0, 934],\n",
       " ['felt', 0.0, 939],\n",
       " ['experience', 0.0, 941],\n",
       " ['energy', 0.0, 945],\n",
       " ['eld', 0.0, 946],\n",
       " ['radiating', 0.0, 947],\n",
       " ['workinpotential', 0.0, 950],\n",
       " ['resistance', 0.0, 951],\n",
       " ['repelling', 0.0, 954],\n",
       " ['force', 0.0, 955],\n",
       " ['negative', 0.0, 957],\n",
       " ['aim', 0.0, 959],\n",
       " ['shove', 0.0, 962],\n",
       " ['us', 0.0, 963],\n",
       " ['away', 0.0, 964],\n",
       " ['distract', 0.0, 965],\n",
       " ['us', 0.0, 966],\n",
       " ['prevent', 0.0, 967],\n",
       " ['us', 0.0, 968],\n",
       " ['work', 0.0, 972],\n",
       " ['resistance', 0.0, 973],\n",
       " ['insidiousresistance', 0.0, 975],\n",
       " ['tell', 0.0, 977],\n",
       " ['anything', 0.0, 979],\n",
       " ['keep', 0.0, 981],\n",
       " ['work', 0.0, 986],\n",
       " ['perjure', 0.0, 989],\n",
       " ['fabricate', 0.0, 990],\n",
       " ['falsify', 0.0, 991],\n",
       " ['seduce', 0.0, 992],\n",
       " ['bully', 0.0, 993],\n",
       " ['cajole', 0.0, 994],\n",
       " ['resistance', 0.0, 996],\n",
       " ['protean', 0.0, 998],\n",
       " ['assume', 0.0, 1001],\n",
       " ['form', 0.0, 1003],\n",
       " ['thats', 0.0, 1005],\n",
       " ['takes', 0.0, 1009],\n",
       " ['deceive', 0.0, 1011],\n",
       " ['resistance', 0.0, 1013],\n",
       " ['reason', 0.0, 1015],\n",
       " ['like', 0.0, 1018],\n",
       " ['lawyer', 0.0, 1020],\n",
       " ['jam', 0.0, 1022],\n",
       " ['nine', 0.0, 1024],\n",
       " ['millimeter', 0.0, 1025],\n",
       " ['face', 0.0, 1028],\n",
       " ['like', 0.0, 1029],\n",
       " ['stickup', 0.0, 1031],\n",
       " ['man', 0.0, 1032],\n",
       " ['resistance', 0.0, 1033],\n",
       " ['conscience', 0.0, 1036],\n",
       " ['pledge', 0.0, 1039],\n",
       " ['anything', 0.0, 1040],\n",
       " ['get', 0.0, 1042],\n",
       " ['deal', 0.0, 1044],\n",
       " ['doublecross', 0.0, 1046],\n",
       " ['soon', 0.0, 1049],\n",
       " ['back', 0.0, 1052],\n",
       " ['turned', 0.0, 1054],\n",
       " ['take', 0.0, 1058],\n",
       " ['resistance', 0.0, 1059],\n",
       " ['word', 0.0, 1062],\n",
       " ['deserve', 0.0, 1064],\n",
       " ['everything', 0.0, 1065],\n",
       " ['get', 0.0, 1067],\n",
       " ['resistance', 0.0, 1068],\n",
       " ['always', 0.0, 1070],\n",
       " ['lying', 0.0, 1071],\n",
       " ['always', 0.0, 1073],\n",
       " ['full', 0.0, 1074],\n",
       " ['shit', 0.0, 1076],\n",
       " ['resistance', 0.0, 1077],\n",
       " ['impersonalresistance', 0.0, 1079],\n",
       " ['get', 0.0, 1084],\n",
       " ['personally', 0.0, 1086],\n",
       " ['doesnt', 0.0, 1088],\n",
       " ['know', 0.0, 1089],\n",
       " ['doesnt', 0.0, 1094],\n",
       " ['care', 0.0, 1095],\n",
       " ['resistance', 0.0, 1096],\n",
       " ['force', 0.0, 1099],\n",
       " ['nature', 0.0, 1101],\n",
       " ['acts', 0.0, 1103],\n",
       " ['objectively', 0.0, 1105],\n",
       " ['ough', 0.0, 1106],\n",
       " ['feels', 0.0, 1108],\n",
       " ['malevolent', 0.0, 1109],\n",
       " ['resistance', 0.0, 1110],\n",
       " ['fact', 0.0, 1112],\n",
       " ['operates', 0.0, 1113],\n",
       " ['indi', 0.0, 1116],\n",
       " ['erence', 0.0, 1117],\n",
       " ['rain', 0.0, 1119],\n",
       " ['transits', 0.0, 1121],\n",
       " ['heavens', 0.0, 1123],\n",
       " ['laws', 0.0, 1127],\n",
       " ['stars', 0.0, 1129],\n",
       " ['marshal', 0.0, 1132],\n",
       " ['forces', 0.0, 1134],\n",
       " ['combat', 0.0, 1136],\n",
       " ['resistance', 0.0, 1137],\n",
       " ['must', 0.0, 1139],\n",
       " ['remember', 0.0, 1140],\n",
       " ['steven', 0.0, 1142],\n",
       " ['p', 0.0, 1143],\n",
       " ['elddo', 0.0, 1144],\n",
       " ['e', 0.0, 1145],\n",
       " ['work', 0.0, 1146],\n",
       " ['89resistance', 0.0, 1147],\n",
       " ['infalliblelike', 0.0, 1149],\n",
       " ['magnetized', 0.0, 1151],\n",
       " ['need', 0.0, 1152],\n",
       " ['oating', 0.0, 1153],\n",
       " ['surface', 0.0, 1156],\n",
       " ['oil', 0.0, 1158],\n",
       " ['resistance', 0.0, 1159],\n",
       " ['unfailingly', 0.0, 1161],\n",
       " ['point', 0.0, 1162],\n",
       " ['true', 0.0, 1164],\n",
       " ['northmeaning', 0.0, 1165],\n",
       " ['calling', 0.0, 1167],\n",
       " ['action', 0.0, 1170],\n",
       " ['wants', 0.0, 1173],\n",
       " ['stop', 0.0, 1175],\n",
       " ['us', 0.0, 1176],\n",
       " ['use', 0.0, 1181],\n",
       " ['use', 0.0, 1185],\n",
       " ['compass', 0.0, 1189],\n",
       " ['navigate', 0.0, 1192],\n",
       " ['resistance', 0.0, 1194],\n",
       " ['letting', 0.0, 1195],\n",
       " ['guide', 0.0, 1197],\n",
       " ['us', 0.0, 1198],\n",
       " ['calling', 0.0, 1201],\n",
       " ['purpose', 0.0, 1203],\n",
       " ['must', 0.0, 1206],\n",
       " ['follow', 0.0, 1207],\n",
       " ['others', 0.0, 1210],\n",
       " ['rule', 0.0, 1211],\n",
       " ['thumb', 0.0, 1213],\n",
       " ['e', 0.0, 1214],\n",
       " ['important', 0.0, 1216],\n",
       " ['call', 0.0, 1218],\n",
       " ['action', 0.0, 1220],\n",
       " ['souls', 0.0, 1224],\n",
       " ['evolution', 0.0, 1225],\n",
       " ['resistance', 0.0, 1228],\n",
       " ['feel', 0.0, 1231],\n",
       " ['toward', 0.0, 1232],\n",
       " ['pursuing', 0.0, 1233],\n",
       " ['resistance', 0.0, 1235],\n",
       " ['universal', 0.0, 1237],\n",
       " ['wrong', 0.0, 1239],\n",
       " ['think', 0.0, 1242],\n",
       " ['ones', 0.0, 1246],\n",
       " ['struggling', 0.0, 1247],\n",
       " ['resistance', 0.0, 1249],\n",
       " ['everyone', 0.0, 1250],\n",
       " ['body', 0.0, 1254],\n",
       " ['experiences', 0.0, 1255],\n",
       " ['resistance', 0.0, 1256],\n",
       " ['resistance', 0.0, 1257],\n",
       " ['never', 0.0, 1258],\n",
       " ['sleeps', 0.0, 1259],\n",
       " ['henry', 0.0, 1260],\n",
       " ['fonda', 0.0, 1261],\n",
       " ['still', 0.0, 1263],\n",
       " ['throwing', 0.0, 1264],\n",
       " ['stage', 0.0, 1268],\n",
       " ['perfor', 0.0, 1269],\n",
       " ['mance', 0.0, 1271],\n",
       " ['even', 0.0, 1272],\n",
       " ['seventy', 0.0, 1276],\n",
       " ['words', 0.0, 1280],\n",
       " ['fear', 0.0, 1281],\n",
       " ['doesnt', 0.0, 1282],\n",
       " ['go', 0.0, 1283],\n",
       " ['away', 0.0, 1284],\n",
       " ['e', 0.0, 1285],\n",
       " ['warrior', 0.0, 1286],\n",
       " ['artist', 0.0, 1289],\n",
       " ['live', 0.0, 1290],\n",
       " ['code', 0.0, 1294],\n",
       " ['necessity', 0.0, 1296],\n",
       " ['dictates', 0.0, 1298],\n",
       " ['battle', 0.0, 1301],\n",
       " ['must', 0.0, 1302],\n",
       " ['fought', 0.0, 1304],\n",
       " ['anew', 0.0, 1305],\n",
       " ['every', 0.0, 1306],\n",
       " ['day', 0.0, 1307],\n",
       " ['resistance', 0.0, 1308],\n",
       " ['plays', 0.0, 1309],\n",
       " ['keeps', 0.0, 1311],\n",
       " ['resistances', 0.0, 1312],\n",
       " ['goal', 0.0, 1313],\n",
       " ['wound', 0.0, 1317],\n",
       " ['disable', 0.0, 1319],\n",
       " ['resistance', 0.0, 1320],\n",
       " ['aims', 0.0, 1321],\n",
       " ['kill', 0.0, 1323],\n",
       " ['target', 0.0, 1325],\n",
       " ['epicenter', 0.0, 1328],\n",
       " ['genius', 0.0, 1333],\n",
       " ['soul', 0.0, 1335],\n",
       " ['unique', 0.0, 1337],\n",
       " ['priceless', 0.0, 1339],\n",
       " ['gi', 0.0, 1340],\n",
       " ['put', 0.0, 1343],\n",
       " ['earth', 0.0, 1346],\n",
       " ['give', 0.0, 1348],\n",
       " ['one', 0.0, 1352],\n",
       " ['else', 0.0, 1353],\n",
       " ['us', 0.0, 1356],\n",
       " ['resistance', 0.0, 1357],\n",
       " ['means', 0.0, 1358],\n",
       " ['business', 0.0, 1359],\n",
       " ['ght', 0.0, 1362],\n",
       " ['war', 0.0, 1368],\n",
       " ['death', 0.0, 1371],\n",
       " ['steven', 0.0, 1372],\n",
       " ['p', 0.0, 1373],\n",
       " ['elddo', 0.0, 1374],\n",
       " ['e', 0.0, 1375],\n",
       " ['work', 0.0, 1376],\n",
       " ['1011rational', 0.0, 1377],\n",
       " ['thought', 0.0, 1378],\n",
       " ['next', 0.0, 1379],\n",
       " ['resistance', 0.0, 1381],\n",
       " ['rational', 0.0, 1382],\n",
       " ['thought', 0.0, 1383],\n",
       " ['artist', 0.0, 1386],\n",
       " ['entrepre', 0.0, 1388],\n",
       " ['neurs', 0.0, 1389],\n",
       " ['worst', 0.0, 1390],\n",
       " ['enemy', 0.0, 1391],\n",
       " ['bad', 0.0, 1392],\n",
       " ['things', 0.0, 1393],\n",
       " ['happen', 0.0, 1394],\n",
       " ['employ', 0.0, 1397],\n",
       " ['rational', 0.0, 1398],\n",
       " ['thought', 0.0, 1399],\n",
       " ['rational', 0.0, 1401],\n",
       " ['thought', 0.0, 1402],\n",
       " ['comes', 0.0, 1403],\n",
       " ['ego', 0.0, 1406],\n",
       " ['instead', 0.0, 1407],\n",
       " ['want', 0.0, 1409],\n",
       " ['work', 0.0, 1411],\n",
       " ['self', 0.0, 1414],\n",
       " ['instinct', 0.0, 1418],\n",
       " ['intuition', 0.0, 1420],\n",
       " ['unconscious', 0.0, 1423],\n",
       " ['homer', 0.0, 1424],\n",
       " ['began', 0.0, 1425],\n",
       " ['e', 0.0, 1427],\n",
       " ['iliad', 0.0, 1428],\n",
       " ['e', 0.0, 1430],\n",
       " ['odyssey', 0.0, 1431],\n",
       " ['prayer', 0.0, 1434],\n",
       " ['mu', 0.0, 1437],\n",
       " ['e', 0.0, 1438],\n",
       " ['greeks', 0.0, 1439],\n",
       " ['greatest', 0.0, 1440],\n",
       " ['poet', 0.0, 1441],\n",
       " ['understood', 0.0, 1442],\n",
       " ['genius', 0.0, 1444],\n",
       " ['reside', 0.0, 1447],\n",
       " ['within', 0.0, 1448],\n",
       " ['fallible', 0.0, 1450],\n",
       " ['mortal', 0.0, 1451],\n",
       " ['selfbut', 0.0, 1452],\n",
       " ['came', 0.0, 1453],\n",
       " ['instead', 0.0, 1457],\n",
       " ['source', 0.0, 1460],\n",
       " ['could', 0.0, 1463],\n",
       " ['neither', 0.0, 1464],\n",
       " ['command', 0.0, 1465],\n",
       " ['control', 0.0, 1468],\n",
       " ['invoke', 0.0, 1470],\n",
       " ['artist', 0.0, 1473],\n",
       " ['says', 0.0, 1474],\n",
       " ['trust', 0.0, 1475],\n",
       " ['soup', 0.0, 1477],\n",
       " ['means', 0.0, 1479],\n",
       " ['let', 0.0, 1480],\n",
       " ['go', 0.0, 1481],\n",
       " ['need', 0.0, 1484],\n",
       " ['control', 0.0, 1486],\n",
       " ['cant', 0.0, 1489],\n",
       " ['anyway', 0.0, 1491],\n",
       " ['put', 0.0, 1493],\n",
       " ['faith', 0.0, 1495],\n",
       " ['instead', 0.0, 1497],\n",
       " ['source', 0.0, 1500],\n",
       " ['mystery', 0.0, 1502],\n",
       " ['quantum', 0.0, 1504],\n",
       " ['soup', 0.0, 1505],\n",
       " ['e', 0.0, 1506],\n",
       " ['deeper', 0.0, 1507],\n",
       " ['source', 0.0, 1509],\n",
       " ['work', 0.0, 1511],\n",
       " ['better', 0.0, 1514],\n",
       " ['stu', 0.0, 1516],\n",
       " ['beand', 0.0, 1518],\n",
       " ['transformative', 0.0, 1521],\n",
       " ['us', 0.0, 1526],\n",
       " ['share', 0.0, 1532],\n",
       " ['friends', 0.0, 1535],\n",
       " ['family', 0.0, 1537],\n",
       " ['e', 0.0, 1538],\n",
       " ['problem', 0.0, 1539],\n",
       " ['friends', 0.0, 1541],\n",
       " ['family', 0.0, 1543],\n",
       " ['know', 0.0, 1547],\n",
       " ['us', 0.0, 1548],\n",
       " ['ey', 0.0, 1552],\n",
       " ['invested', 0.0, 1554],\n",
       " ['maintaining', 0.0, 1556],\n",
       " ['us', 0.0, 1557],\n",
       " ['e', 0.0, 1561],\n",
       " ['last', 0.0, 1562],\n",
       " ['thing', 0.0, 1563],\n",
       " ['want', 0.0, 1565],\n",
       " ['remain', 0.0, 1568],\n",
       " ['youre', 0.0, 1573],\n",
       " ['reading', 0.0, 1574],\n",
       " ['book', 0.0, 1576],\n",
       " ['sense', 0.0, 1580],\n",
       " ['inside', 0.0, 1581],\n",
       " ['second', 0.0, 1584],\n",
       " ['self', 0.0, 1585],\n",
       " ['unlived', 0.0, 1587],\n",
       " ['exceptions', 0.0, 1591],\n",
       " ['god', 0.0, 1592],\n",
       " ['bless', 0.0, 1593],\n",
       " ['friends', 0.0, 1595],\n",
       " ['family', 0.0, 1597],\n",
       " ['enemy', 0.0, 1600],\n",
       " ['unmanifested', 0.0, 1603],\n",
       " ['unborn', 0.0, 1606],\n",
       " ['self', 0.0, 1607],\n",
       " ['future', 0.0, 1609],\n",
       " ['prepare', 0.0, 1612],\n",
       " ['make', 0.0, 1615],\n",
       " ['new', 0.0, 1616],\n",
       " ['friends', 0.0, 1617],\n",
       " ['ey', 0.0, 1618],\n",
       " ['appear', 0.0, 1620],\n",
       " ['trust', 0.0, 1621],\n",
       " ['steven', 0.0, 1623],\n",
       " ['p', 0.0, 1624],\n",
       " ['elddo', 0.0, 1625],\n",
       " ['e', 0.0, 1626],\n",
       " ['work', 0.0, 1627],\n",
       " ['1213our', 0.0, 1628],\n",
       " ['allies', 0.0, 1629],\n",
       " ['enough', 0.0, 1630],\n",
       " ['antagonists', 0.0, 1635],\n",
       " ['arrayed', 0.0, 1636],\n",
       " ['us', 0.0, 1638],\n",
       " ['lets', 0.0, 1639],\n",
       " ['consider', 0.0, 1640],\n",
       " ['champions', 0.0, 1642],\n",
       " ['side', 0.0, 1645],\n",
       " ['1', 0.0, 1646],\n",
       " ['stupidity', 0.0, 1647],\n",
       " ['2', 0.0, 1648],\n",
       " ['stubbornness', 0.0, 1649],\n",
       " ['3', 0.0, 1651],\n",
       " ['blind', 0.0, 1652],\n",
       " ['faith', 0.0, 1653],\n",
       " ['4', 0.0, 1655],\n",
       " ['passion', 0.0, 1656],\n",
       " ['5', 0.0, 1658],\n",
       " ['assistance', 0.0, 1659],\n",
       " ['opposite', 0.0, 1661],\n",
       " ['resistance', 0.0, 1663],\n",
       " ['6', 0.0, 1665],\n",
       " ['friends', 0.0, 1666],\n",
       " ['family', 0.0, 1668],\n",
       " ['stay', 0.0, 1669],\n",
       " ['stupid', 0.0, 1670],\n",
       " ['e', 0.0, 1671],\n",
       " ['three', 0.0, 1672],\n",
       " ['dumbest', 0.0, 1673],\n",
       " ['guys', 0.0, 1674],\n",
       " ['think', 0.0, 1677],\n",
       " ['charles', 0.0, 1679],\n",
       " ['lindbergh', 0.0, 1680],\n",
       " ['steve', 0.0, 1681],\n",
       " ['jobs', 0.0, 1682],\n",
       " ['winston', 0.0, 1683],\n",
       " ['churchill', 0.0, 1684],\n",
       " ['smart', 0.0, 1688],\n",
       " ['person', 0.0, 1689],\n",
       " ['understood', 0.0, 1692],\n",
       " ['impossibly', 0.0, 1694],\n",
       " ['arduous', 0.0, 1695],\n",
       " ['tasks', 0.0, 1698],\n",
       " ['set', 0.0, 1701],\n",
       " ['would', 0.0, 1704],\n",
       " ['pulled', 0.0, 1706],\n",
       " ['plug', 0.0, 1708],\n",
       " ['even', 0.0, 1711],\n",
       " ['began', 0.0, 1712],\n",
       " ['ignorance', 0.0, 1713],\n",
       " ['arrogance', 0.0, 1715],\n",
       " ['artist', 0.0, 1718],\n",
       " ['entrepreneurs', 0.0, 1720],\n",
       " ['dispensable', 0.0, 1722],\n",
       " ['allies', 0.0, 1723],\n",
       " ['must', 0.0, 1725],\n",
       " ['clueless', 0.0, 1727],\n",
       " ['enough', 0.0, 1728],\n",
       " ['idea', 0.0, 1732],\n",
       " ['di', 0.0, 1735],\n",
       " ['cult', 0.0, 1736],\n",
       " ['enterprise', 0.0, 1738],\n",
       " ['going', 0.0, 1740],\n",
       " ['beand', 0.0, 1742],\n",
       " ['cocky', 0.0, 1743],\n",
       " ['enough', 0.0, 1744],\n",
       " ['believe', 0.0, 1746],\n",
       " ['pull', 0.0, 1749],\n",
       " ['anyway', 0.0, 1750],\n",
       " ['achieve', 0.0, 1754],\n",
       " ['state', 0.0, 1756],\n",
       " ['mind', 0.0, 1758],\n",
       " ['staying', 0.0, 1760],\n",
       " ['stupid', 0.0, 1761],\n",
       " ['allowing', 0.0, 1764],\n",
       " ['think', 0.0, 1767],\n",
       " ['child', 0.0, 1769],\n",
       " ['trouble', 0.0, 1772],\n",
       " ['believing', 0.0, 1773],\n",
       " ['unbelievable', 0.0, 1775],\n",
       " ['genius', 0.0, 1779],\n",
       " ['madman', 0.0, 1782],\n",
       " ['big', 0.0, 1790],\n",
       " ['brains', 0.0, 1791],\n",
       " ['tiny', 0.0, 1794],\n",
       " ['hearts', 0.0, 1795],\n",
       " ['doubt', 0.0, 1797],\n",
       " ['overthink', 0.0, 1799],\n",
       " ['hesitate', 0.0, 1801],\n",
       " ['dont', 0.0, 1802],\n",
       " ['think', 0.0, 1803],\n",
       " ['act', 0.0, 1804],\n",
       " ['always', 0.0, 1807],\n",
       " ['revise', 0.0, 1808],\n",
       " ['revisit', 0.0, 1810],\n",
       " ['weve', 0.0, 1812],\n",
       " ['acted', 0.0, 1813],\n",
       " ['accomplish', 0.0, 1817],\n",
       " ['nothing', 0.0, 1818],\n",
       " ['act', 0.0, 1821],\n",
       " ['stubborn', 0.0, 1823],\n",
       " ['commit', 0.0, 1826],\n",
       " ['action', 0.0, 1828],\n",
       " ['worst', 0.0, 1830],\n",
       " ['thing', 0.0, 1831],\n",
       " ['stop', 0.0, 1837],\n",
       " ['keep', 0.0, 1840],\n",
       " ['us', 0.0, 1841],\n",
       " ['stopping', 0.0, 1843],\n",
       " ['plain', 0.0, 1844],\n",
       " ['old', 0.0, 1845],\n",
       " ['stubbornness', 0.0, 1846],\n",
       " ...]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['pages'] = df.index\n",
    "#this cell runs only one time\n",
    "# if you want to run again go back from the previous cell\n",
    "\n",
    "#files_list = files()\n",
    "#stop = stopwords.words('english')\n",
    "#for i in  files_list:\n",
    "    # convert to lower case\n",
    "  #  df[i] = df[i].str.lower()\n",
    "    # removing puncituations\n",
    " #   df[i] = df[i].str.replace('[^\\w\\s]','')\n",
    "    #convert to string\n",
    "#    df[i] = df[i].apply(str)\n",
    "    \n",
    "\n",
    "    #splitting new lines on '\\n'\n",
    "#    for j in range(len(df.index)):\n",
    "#        df[i][j] = df[i][j].replace('\\r', ' ').split('\\n')\n",
    "    #df[i] = df[i].apply(literal_eval)    \n",
    "    #df[i] = df[i].apply(str)\n",
    "\n",
    "    #df.explode(i)\n",
    "\n",
    "# convert list of pd.Series then stack it\n",
    "#    df = (df\n",
    "     #.set_index(['issue_key','date','pkey','case_count'])['component']\n",
    "#     .apply(pd.Series)\n",
    "#     .stack()\n",
    "#     .reset_index()\n",
    "#     .reset_index(level=0, drop=True))\n",
    "     #.rename(columns={0:'component'}))\n",
    "\n",
    "    #convert to string\n",
    "    #df[i] = df[i].apply(str)\n",
    "    \n",
    "    # tokenize\n",
    "    #df[i] = df[i].apply(nltk.word_tokenize)\n",
    "    \n",
    "    # removing stop words\n",
    "    #df[i].apply(lambda x: [item for item in x if item not in stop])\n",
    "    \n",
    "    #df[i]= df[i].str.splitlines()\n",
    "    #df[i].apply(lambda x: (word for word in x.str.splitlines()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
